{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Connect drive"
      ],
      "metadata": {
        "id": "Kelqsqc2niTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9VXSd2z7GUv",
        "outputId": "9c5102f1-2cd2-4b09-ca25-d483957521bf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Vocab"
      ],
      "metadata": {
        "id": "qoAMGjvgnkbG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.nn import functional\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, paths):\n",
        "        self.chars = None\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self.load_dataset(paths)\n",
        "\n",
        "    def load_dataset(self, paths):\n",
        "        if isinstance(paths, str):\n",
        "            paths = [paths]\n",
        "\n",
        "        for path in paths:\n",
        "            with open(path, 'r', encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split('\\t')\n",
        "                    if len(parts) != 2:\n",
        "                        continue\n",
        "                    img_path, label = parts\n",
        "                    self.image_paths.append(img_path)\n",
        "                    self.labels.append(label)\n",
        "        print(f\"Total {len(self.image_paths)} images loaded from {len(paths)} file(s)\")\n",
        "\n",
        "    def create_vocab(self):\n",
        "        letters = \"\".join(self.labels)\n",
        "        unique_chars = sorted(set(letters))\n",
        "\n",
        "        self.chars = \"\".join(unique_chars)\n",
        "\n",
        "        self.char_2_idx = {char: idx + 2 for idx, char in enumerate(self.chars)}\n",
        "        self.blank_index = 1  # index cho CTC blank\n",
        "        self.idx_2_char = {idx: char for char, idx in self.char_2_idx.items()}\n",
        "\n",
        "        print(f\"Vocab size (with blank): {len(self.chars) + 1}\")\n",
        "        print(f\"char_2_idx: {self.char_2_idx}\")\n",
        "\n",
        "    def encode(self, input_sequence):\n",
        "        max_label_len = max(len(label) for label in self.labels)\n",
        "        encoded = torch.tensor(\n",
        "            [self.char_2_idx.get(char, 0) for char in input_sequence], dtype=torch.long  #  -> 0 (pad)\n",
        "        )\n",
        "        label_len = len(encoded)\n",
        "        lengths = torch.tensor(label_len, dtype=torch.long)\n",
        "        padded = functional.pad(encoded, (0, max_label_len - label_len))\n",
        "        return padded, lengths\n",
        "\n",
        "    def decode(self, encode_sequences):\n",
        "        decode_sequences = []\n",
        "\n",
        "        for seq in encode_sequences:\n",
        "            decode_label = []\n",
        "            prev_token = None\n",
        "            if isinstance(seq, torch.Tensor):\n",
        "                seq = seq.tolist()\n",
        "\n",
        "            for token in seq:\n",
        "                if token != self.blank_index and token != 0 and token != prev_token:\n",
        "                    char = self.idx_2_char.get(token, '')\n",
        "                    decode_label.append(char)\n",
        "                prev_token = token\n",
        "            decode_sequences.append(''.join(decode_label))\n",
        "        return decode_sequences\n",
        "\n",
        "    def idx_2_labels(self, labels):\n",
        "        idx_2_labels = []\n",
        "        for label in labels:\n",
        "            if isinstance(label, torch.Tensor):\n",
        "                label = label.tolist()\n",
        "            chars = [self.idx_2_char.get(idx, '') for idx in label if idx != 0]\n",
        "            idx_2_labels.append(''.join(chars))\n",
        "        return idx_2_labels\n",
        "\n",
        "vocab = Vocab([\n",
        "    \"/content/drive/MyDrive/scene-text-ocr/train.txt\",\n",
        "    \"/content/drive/MyDrive/scene-text-ocr/val.txt\",\n",
        "    \"/content/drive/MyDrive/scene-text-ocr/test.txt\"\n",
        "])\n",
        "vocab.create_vocab()\n"
      ],
      "metadata": {
        "id": "zg-9C2779Yjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset"
      ],
      "metadata": {
        "id": "Y4aExAP1np_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class OCRDataSet(Dataset):\n",
        "    def __init__(self, mode = 'train', label_encoder = None, transform= None):\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self.label_encoder = label_encoder\n",
        "        if mode == 'train':\n",
        "            path = \"/content/drive/MyDrive/scene-text-ocr/train.txt\"\n",
        "        elif mode == 'val':\n",
        "            path = \"/content/drive/MyDrive/scene-text-ocr/val.txt\"\n",
        "        elif mode == 'test':\n",
        "            path = \"/content/drive/MyDrive/scene-text-ocr/test.txt\"\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                label = line.strip().split('\\t')[1]\n",
        "                img_path = line.strip().split('\\t')[0]\n",
        "                self.image_paths.append(img_path)\n",
        "                self.labels.append(label)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "        image = Image.open(os.path.join(image_path)).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.label_encoder:\n",
        "            encoded_label, label_len = self.label_encoder(label)\n",
        "            return image, encoded_label, label_len\n",
        "        else:\n",
        "            exit(-1)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    vocab = Vocab([\n",
        "        \"/content/drive/MyDrive/scene-text-ocr/train.txt\",\n",
        "        \"/content/drive/MyDrive/scene-text-ocr/val.txt\",\n",
        "        \"/content/drive/MyDrive/scene-text-ocr/test.txt\"\n",
        "    ])\n",
        "    vocab.create_vocab()\n",
        "    hw = OCRDataSet(mode = 'train', label_encoder = vocab.encode)\n"
      ],
      "metadata": {
        "id": "GhrmKhwo9rdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Backbone"
      ],
      "metadata": {
        "id": "OSyucnAinsgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet34\n",
        "\n",
        "class ResNet34Backbone(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        base_model = resnet34(pretrained=pretrained)\n",
        "\n",
        "        old_conv1 = base_model.conv1\n",
        "\n",
        "        new_conv1 = nn.Conv2d(\n",
        "            in_channels=1,\n",
        "            out_channels=old_conv1.out_channels,\n",
        "            kernel_size=old_conv1.kernel_size,\n",
        "            stride=old_conv1.stride,\n",
        "            padding=old_conv1.padding,\n",
        "            bias=old_conv1.bias is not None,\n",
        "        )\n",
        "\n",
        "        if pretrained:\n",
        "            with torch.no_grad():\n",
        "                new_conv1.weight = nn.Parameter(old_conv1.weight.mean(dim=1, keepdim=True))\n",
        "\n",
        "        base_model.conv1 = new_conv1\n",
        "\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            base_model.conv1,\n",
        "            base_model.bn1,\n",
        "            base_model.relu,\n",
        "            base_model.maxpool,\n",
        "            base_model.layer1,\n",
        "            base_model.layer2,\n",
        "            base_model.layer3,\n",
        "            base_model.layer4,\n",
        "        )\n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, None))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)  # (B, 512, H', W')\n",
        "        x = self.pool(x)               # (B, 512, 1, W)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "jYajn1tzBoO5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sequence head"
      ],
      "metadata": {
        "id": "AuiEK4EcnuPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, n_layers, dropout=0.3):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=512,\n",
        "            hidden_size=hidden_size,\n",
        "            bidirectional=True,\n",
        "            num_layers=n_layers,\n",
        "            dropout=dropout if n_layers > 1 else 0,\n",
        "        )\n",
        "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
        "        self.out = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, vocab_size),\n",
        "            nn.LogSoftmax(dim=2),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 3, 1, 2)  # (B, W, C, 1)\n",
        "        x = x.view(x.size(0), x.size(1), -1)  # (B, W, 768)\n",
        "        x, _ = self.lstm(x)  # (B, W, 2*hidden)\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.out(x)\n",
        "        x = x.permute(1, 0, 2)  # (W, B, Class)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "s--f0ZQ9ADPZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "eqe1QFc9nvy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class OCRModel(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, n_layers, dropout=0.3, pretrained_backbone=True):\n",
        "        super(OCRModel, self).__init__()\n",
        "        self.backbone = ResNet34Backbone(pretrained=pretrained_backbone)\n",
        "        self.lstm = BiLSTM(\n",
        "            vocab_size=vocab_size,\n",
        "            hidden_size=hidden_size,\n",
        "            n_layers=n_layers,\n",
        "            dropout=dropout,\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.lstm(x)\n",
        "        return x\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = OCRModel(vocab_size=38, hidden_size=512, n_layers=2)\n",
        "    dummy_input = torch.randn(1, 1, 100, 420)  # (B, C, H, W)\n",
        "    output = model(dummy_input)\n",
        "    print(\"Final output shape:\", output.shape)\n"
      ],
      "metadata": {
        "id": "Yx98UqDHAI-_",
        "outputId": "8a3efddd-84c2-4dd1-daf7-a0c284d7b898",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final output shape: torch.Size([14, 1, 38])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Trainer"
      ],
      "metadata": {
        "id": "IPa6hPtBnyiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer"
      ],
      "metadata": {
        "id": "prlWYnSZFYXs",
        "outputId": "03c36b49-bb78-428b-ac84-30a8dc268125",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.11/dist-packages (4.0.0)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.2.1)\n",
            "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from jiwer) (3.13.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from jiwer import cer\n",
        "from tqdm import tqdm\n",
        "\n",
        "def calculate_cer(preds, target):\n",
        "    total_cer = 0\n",
        "    for pred, target in zip(preds, target):\n",
        "        total_cer += cer(target, pred)\n",
        "    return total_cer / len(preds)\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs, vocab_util, log_dir, save_path):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        self.device = device\n",
        "        self.epochs = epochs\n",
        "        self.vocab_util = vocab_util\n",
        "        self.save_path = save_path\n",
        "        self.log_dir = log_dir\n",
        "        os.makedirs(self.save_path, exist_ok=True)\n",
        "        os.makedirs(self.log_dir, exist_ok=True)\n",
        "        self.train_log_path = os.path.join(self.log_dir, \"train_log.txt\")\n",
        "        self.val_log_path = os.path.join(self.log_dir, \"val_log.txt\")\n",
        "        self.best_acc = 1e10\n",
        "        self.start_epoch = 0\n",
        "\n",
        "    def load_checkpoint(self, path):\n",
        "        if os.path.exists(path):\n",
        "            checkpoint = torch.load(path)\n",
        "            self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "            self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "            self.scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
        "            self.start_epoch = checkpoint[\"epoch\"]\n",
        "            self.best_acc = checkpoint[\"best_acc\"]\n",
        "            print(f\"Loaded checkpoint '{path}' (epoch {self.start_epoch})\")\n",
        "\n",
        "    def save_checkpoint(self, epoch, best_acc, is_best=False):\n",
        "        checkpoint = {\n",
        "            \"epoch\": epoch,\n",
        "            \"best_acc\": best_acc,\n",
        "            \"model_state_dict\": self.model.state_dict(),\n",
        "            \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
        "            \"scheduler_state_dict\": self.scheduler.state_dict(),\n",
        "        }\n",
        "        torch.save(checkpoint, os.path.join(self.save_path, \"last_model.pt\"))\n",
        "        if is_best:\n",
        "            torch.save(checkpoint, os.path.join(self.save_path, \"best_model.pt\"))\n",
        "\n",
        "    def log(self, text, train=True):\n",
        "        path = self.train_log_path if train else self.val_log_path\n",
        "        with open(path, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(text + \"\\n\")\n",
        "\n",
        "    def fit(self):\n",
        "        checkpoint_path = os.path.join(self.save_path, \"last_model.pt\")\n",
        "        self.load_checkpoint(checkpoint_path)\n",
        "        for epoch in range(self.start_epoch, self.epochs):\n",
        "            self.model.train()\n",
        "            batch_train_losses = []\n",
        "            train_preds, train_targets = [], []\n",
        "            pbar_train = tqdm(self.train_loader, desc=f\"Train Epoch {epoch}\", leave=False)\n",
        "            for inputs, encoded_labels, labels_len in pbar_train:\n",
        "                inputs = inputs.to(self.device)\n",
        "                encoded_labels = encoded_labels.to(self.device)\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "                outputs = self.model(inputs) # (T, B, C)\n",
        "\n",
        "                logit_lens = torch.full(\n",
        "                    size=(outputs.size(1),),\n",
        "                    fill_value=outputs.size(0),\n",
        "                    dtype=torch.long\n",
        "                ).to(self.device)\n",
        "\n",
        "                loss = self.criterion(outputs, encoded_labels, logit_lens, labels_len)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 5)\n",
        "                self.optimizer.step()\n",
        "                batch_train_losses.append(loss.item())\n",
        "\n",
        "                pred_labels = torch.argmax(outputs, dim=2).permute(1, 0)  # (B, T)\n",
        "                pred_texts = self.vocab_util.decode(pred_labels)\n",
        "                train_preds.extend(pred_texts)\n",
        "\n",
        "                target_texts = self.vocab_util.idx_2_labels(encoded_labels)\n",
        "                train_targets.extend(target_texts)\n",
        "\n",
        "                batch_cer = calculate_cer(pred_texts, target_texts)\n",
        "                pbar_train.set_postfix(loss=loss.item(), cer=batch_cer)\n",
        "\n",
        "            train_loss_avg = sum(batch_train_losses) / len(batch_train_losses)\n",
        "            train_cer = calculate_cer(train_preds, train_targets)\n",
        "\n",
        "            train_log = f\"Epoch {epoch} - Train Loss: {train_loss_avg:.4f}, Train CER: {train_cer:.4f}\"\n",
        "            # print(train_log)\n",
        "            self.log(train_log, train=True)\n",
        "\n",
        "            val_loss, val_cer = self.evaluate(epoch)\n",
        "\n",
        "            val_log = f\"Epoch {epoch} - Val Loss: {val_loss:.4f}, Val CER: {val_cer:.4f}\"\n",
        "            # print(val_log)\n",
        "            self.log(val_log, train=False)\n",
        "\n",
        "            self.save_checkpoint(epoch + 1, self.best_acc, is_best=False)\n",
        "\n",
        "            if val_cer < self.best_acc:\n",
        "                self.best_acc = val_cer\n",
        "                self.save_checkpoint(epoch + 1, self.best_acc, is_best=True)\n",
        "\n",
        "            self.scheduler.step()\n",
        "\n",
        "    def evaluate(self, epoch=None):\n",
        "        self.model.eval()\n",
        "        losses = []\n",
        "        val_preds, val_targets = [], []\n",
        "        pbar_val = tqdm(self.val_loader, desc=f\"Val Epoch {epoch}\" if epoch is not None else \"Validation\", leave=False)\n",
        "        with torch.no_grad():\n",
        "            for inputs, encoded_labels, labels_len in pbar_val:\n",
        "                inputs = inputs.to(self.device)\n",
        "                encoded_labels = encoded_labels.to(self.device)\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                logit_lens = torch.full(\n",
        "                    size=(outputs.size(1),),\n",
        "                    fill_value=outputs.size(0),\n",
        "                    dtype=torch.long\n",
        "                ).to(self.device)\n",
        "\n",
        "                loss = self.criterion(outputs, encoded_labels, logit_lens, labels_len)\n",
        "                losses.append(loss.item())\n",
        "\n",
        "                pred_labels = torch.argmax(outputs, dim=2).permute(1, 0)\n",
        "                pred_texts = self.vocab_util.decode(pred_labels)\n",
        "                val_preds.extend(pred_texts)\n",
        "\n",
        "                target_texts = self.vocab_util.idx_2_labels(encoded_labels)\n",
        "                val_targets.extend(target_texts)\n",
        "\n",
        "                batch_cer = calculate_cer(pred_texts, target_texts)\n",
        "                pbar_val.set_postfix(loss=loss.item(), cer=batch_cer)\n",
        "\n",
        "        loss_avg = sum(losses) / len(losses)\n",
        "        cer_score = calculate_cer(val_preds, val_targets)\n",
        "        return loss_avg, cer_score\n"
      ],
      "metadata": {
        "id": "rx3dMaGkFdiu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing and transform"
      ],
      "metadata": {
        "id": "KYpp8FlJn3Ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from skimage.morphology import skeletonize\n",
        "import random\n",
        "import albumentations as A\n",
        "from torchvision import transforms\n",
        "\n",
        "data_transforms = {\n",
        "    \"train\": transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((100, 420)),\n",
        "            transforms.ColorJitter(\n",
        "                brightness=0.5,\n",
        "                contrast=0.5,\n",
        "                saturation=0.5,\n",
        "            ),\n",
        "            transforms.Grayscale(\n",
        "                num_output_channels=1,\n",
        "            ),\n",
        "            transforms.GaussianBlur(3),\n",
        "            transforms.RandomAffine(\n",
        "                degrees=1,\n",
        "                shear=1,\n",
        "            ),\n",
        "            transforms.RandomPerspective(\n",
        "                distortion_scale=0.3,\n",
        "                p=0.5,\n",
        "                interpolation=3,\n",
        "            ),\n",
        "            transforms.RandomRotation(degrees=2),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,)),\n",
        "        ]\n",
        "    ),\n",
        "    \"val\": transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize((100, 420)),\n",
        "            transforms.Grayscale(num_output_channels=1),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,)),\n",
        "        ]\n",
        "    ),\n",
        "}"
      ],
      "metadata": {
        "id": "MSUfljmJbQWK"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train"
      ],
      "metadata": {
        "id": "gSVrpETVn-Pl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = OCRDataSet(\n",
        "    mode = 'train',\n",
        "    label_encoder = vocab.encode,\n",
        "    transform = data_transforms['train']\n",
        ")\n",
        "\n",
        "val_dataset = OCRDataSet(\n",
        "    mode = 'val',\n",
        "    label_encoder = vocab.encode,\n",
        "    transform = data_transforms['val']\n",
        ")\n",
        "\n",
        "train_batch_size = 32\n",
        "val_batch_size = 32\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=train_batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=8,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=val_batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=8,\n",
        "    pin_memory=True\n",
        ")"
      ],
      "metadata": {
        "id": "67aWHEbWFwft"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "epochs = 100\n",
        "learning_rate = 1e-3\n",
        "weight_decay = 1e-5\n",
        "step_size = epochs * 0.4\n",
        "gamma = 0.1\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = OCRModel(\n",
        "    vocab_size=38,\n",
        "    hidden_size=512,\n",
        "    n_layers=2,\n",
        ")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "ctc_loss = nn.CTCLoss(blank=1, zero_infinity=True)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "\n",
        "log_dir = \"/content/drive/MyDrive/logs\"\n",
        "save_path = \"/content/drive/MyDrive/checkpoints\"\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=ctc_loss,\n",
        "    optimizer=optimizer,\n",
        "    scheduler=scheduler,\n",
        "    device=device,\n",
        "    epochs=epochs,\n",
        "    vocab_util=vocab,\n",
        "    log_dir=log_dir,\n",
        "    save_path= save_path\n",
        ")\n",
        "\n",
        "trainer.fit()"
      ],
      "metadata": {
        "id": "otLsHfz4F1vo"
      },
      "execution_count": 36,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
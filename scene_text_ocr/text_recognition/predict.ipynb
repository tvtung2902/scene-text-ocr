{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#Connect drive",
   "id": "ea744d29e214b6b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#Vocab",
   "id": "767e0fa67fc1c1e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from torch.nn import functional\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, paths):\n",
    "        self.chars = None\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.load_dataset(paths)\n",
    "\n",
    "    def load_dataset(self, paths):\n",
    "        if isinstance(paths, str):\n",
    "            paths = [paths]\n",
    "\n",
    "        for path in paths:\n",
    "            with open(path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) != 2:\n",
    "                        continue\n",
    "                    img_path, label = parts\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.labels.append(label)\n",
    "        print(f\"Total {len(self.image_paths)} images loaded from {len(paths)} file(s)\")\n",
    "\n",
    "    def create_vocab(self):\n",
    "        letters = \"\".join(self.labels)\n",
    "        unique_chars = sorted(set(letters))\n",
    "\n",
    "        self.chars = \"\".join(unique_chars)\n",
    "\n",
    "        self.char_2_idx = {char: idx + 2 for idx, char in enumerate(self.chars)}\n",
    "        self.blank_index = 1  # index cho CTC blank\n",
    "        self.idx_2_char = {idx: char for char, idx in self.char_2_idx.items()}\n",
    "\n",
    "        print(f\"Vocab size (with blank): {len(self.chars) + 1}\")\n",
    "        print(f\"char_2_idx: {self.char_2_idx}\")\n",
    "\n",
    "    def encode(self, input_sequence):\n",
    "        max_label_len = max(len(label) for label in self.labels)\n",
    "        encoded = torch.tensor(\n",
    "            [self.char_2_idx.get(char, 0) for char in input_sequence], dtype=torch.long  #  -> 0 (pad)\n",
    "        )\n",
    "        label_len = len(encoded)\n",
    "        lengths = torch.tensor(label_len, dtype=torch.long)\n",
    "        padded = functional.pad(encoded, (0, max_label_len - label_len))\n",
    "        return padded, lengths\n",
    "\n",
    "    def decode(self, encode_sequences):\n",
    "        decode_sequences = []\n",
    "\n",
    "        for seq in encode_sequences:\n",
    "            decode_label = []\n",
    "            prev_token = None\n",
    "            if isinstance(seq, torch.Tensor):\n",
    "                seq = seq.tolist()\n",
    "\n",
    "            for token in seq:\n",
    "                if token != self.blank_index and token != 0 and token != prev_token:\n",
    "                    char = self.idx_2_char.get(token, '')\n",
    "                    decode_label.append(char)\n",
    "                prev_token = token\n",
    "            decode_sequences.append(''.join(decode_label))\n",
    "        return decode_sequences\n",
    "\n",
    "    def idx_2_labels(self, labels):\n",
    "        idx_2_labels = []\n",
    "        for label in labels:\n",
    "            if isinstance(label, torch.Tensor):\n",
    "                label = label.tolist()\n",
    "            chars = [self.idx_2_char.get(idx, '') for idx in label if idx != 0]\n",
    "            idx_2_labels.append(''.join(chars))\n",
    "        return idx_2_labels\n",
    "\n",
    "vocab = Vocab([\n",
    "    \"/content/drive/MyDrive/scene-text-ocr/train.txt\",\n",
    "    \"/content/drive/MyDrive/scene-text-ocr/val.txt\",\n",
    "    \"/content/drive/MyDrive/scene-text-ocr/test.txt\"\n",
    "])\n",
    "vocab.create_vocab()\n"
   ],
   "id": "65066e854092c8b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#Dataset",
   "id": "c3f728b5b4170da9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class OCRDataSet(Dataset):\n",
    "    def __init__(self, mode = 'train', label_encoder = None, transform= None):\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.label_encoder = label_encoder\n",
    "        if mode == 'train':\n",
    "            path = \"/content/drive/MyDrive/scene-text-ocr/train.txt\"\n",
    "        elif mode == 'val':\n",
    "            path = \"/content/drive/MyDrive/scene-text-ocr/val.txt\"\n",
    "        elif mode == 'test':\n",
    "            path = \"/content/drive/MyDrive/scene-text-ocr/test.txt\"\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                label = line.strip().split('\\t')[1]\n",
    "                img_path = line.strip().split('\\t')[0]\n",
    "                self.image_paths.append(img_path)\n",
    "                self.labels.append(label)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(os.path.join(image_path)).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.label_encoder:\n",
    "            encoded_label, label_len = self.label_encoder(label)\n",
    "            return image, encoded_label, label_len\n",
    "        else:\n",
    "            exit(-1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    vocab = Vocab([\n",
    "        \"/content/drive/MyDrive/scene-text-ocr/train.txt\",\n",
    "        \"/content/drive/MyDrive/scene-text-ocr/val.txt\",\n",
    "        \"/content/drive/MyDrive/scene-text-ocr/test.txt\"\n",
    "    ])\n",
    "    vocab.create_vocab()\n",
    "    hw = OCRDataSet(mode = 'train', label_encoder = vocab.encode)\n"
   ],
   "id": "6741ade5f84d2d31"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#Backbone",
   "id": "55dc9bad1414968c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet34\n",
    "\n",
    "class ResNet34Backbone(nn.Module):\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        base_model = resnet34(pretrained=pretrained)\n",
    "\n",
    "        old_conv1 = base_model.conv1\n",
    "\n",
    "        new_conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=old_conv1.out_channels,\n",
    "            kernel_size=old_conv1.kernel_size,\n",
    "            stride=old_conv1.stride,\n",
    "            padding=old_conv1.padding,\n",
    "            bias=old_conv1.bias is not None,\n",
    "        )\n",
    "\n",
    "        if pretrained:\n",
    "            with torch.no_grad():\n",
    "                new_conv1.weight = nn.Parameter(old_conv1.weight.mean(dim=1, keepdim=True))\n",
    "\n",
    "        base_model.conv1 = new_conv1\n",
    "\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            base_model.conv1,\n",
    "            base_model.bn1,\n",
    "            base_model.relu,\n",
    "            base_model.maxpool,\n",
    "            base_model.layer1,\n",
    "            base_model.layer2,\n",
    "            base_model.layer3,\n",
    "            base_model.layer4,\n",
    "        )\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, None))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)  # (B, 512, H', W')\n",
    "        x = self.pool(x)               # (B, 512, 1, W)\n",
    "        return x\n"
   ],
   "id": "5c4fc462a671b69f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#Sequence head",
   "id": "47b63cc5e7952163"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, n_layers, dropout=0.3):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=512,\n",
    "            hidden_size=hidden_size,\n",
    "            bidirectional=True,\n",
    "            num_layers=n_layers,\n",
    "            dropout=dropout if n_layers > 1 else 0,\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, vocab_size),\n",
    "            nn.LogSoftmax(dim=2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)  # (B, W, C, 1)\n",
    "        x = x.view(x.size(0), x.size(1), -1)  # (B, W, 768)\n",
    "        x, _ = self.lstm(x)  # (B, W, 2*hidden)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.out(x)\n",
    "        x = x.permute(1, 0, 2)  # (W, B, Class)\n",
    "        return x\n"
   ],
   "id": "e7e19b807b7eeeaa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#Model",
   "id": "8b9df53145d48d86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class OCRModel(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, n_layers, dropout=0.3, pretrained_backbone=True):\n",
    "        super(OCRModel, self).__init__()\n",
    "        self.backbone = ResNet34Backbone(pretrained=pretrained_backbone)\n",
    "        self.lstm = BiLSTM(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=hidden_size,\n",
    "            n_layers=n_layers,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.lstm(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = OCRModel(vocab_size=38, hidden_size=512, n_layers=2)\n",
    "    dummy_input = torch.randn(1, 1, 100, 420)  # (B, C, H, W)\n",
    "    output = model(dummy_input)\n",
    "    print(\"Final output shape:\", output.shape)\n"
   ],
   "id": "ed9320378a1395ef"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test",
   "id": "7a77f3b5b327709b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install jiwer",
   "id": "a6d283bd653ee1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from jiwer import cer\n",
    "\n",
    "def calculate_cer(preds, target):\n",
    "    total_cer = 0\n",
    "    for pred, target in zip(preds, target):\n",
    "        total_cer += cer(target, pred)\n",
    "    return total_cer / len(preds)\n"
   ],
   "id": "1b0109e1e5ed8929"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "save_path = \"/content/drive/MyDrive/checkpoints\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = OCRModel(\n",
    "    vocab_size=38,\n",
    "    hidden_size=512,\n",
    "    n_layers=2,\n",
    ")\n",
    "model.to(device)"
   ],
   "id": "6f9c218d00c68f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((100, 420)),\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_dataset = OCRDataSet(\n",
    "    mode = 'test',\n",
    "    label_encoder = vocab.encode,\n",
    "    transform = test_transform\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")"
   ],
   "id": "9616a880fdb9429b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "ctc_loss = nn.CTCLoss(blank=1, zero_infinity=True)",
   "id": "94f1989ec2e9ae47"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Greedy search",
   "id": "7fdc781fc6262993"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_model(model, data_loader, device, criterion, vocab_util, epoch=None):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    all_preds, all_targets = [], []\n",
    "    correct_count = 0\n",
    "\n",
    "    pbar = tqdm(data_loader, desc=f\"Eval Epoch {epoch}\" if epoch is not None else \"Evaluation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for inputs, encoded_labels, labels_len in pbar:\n",
    "            inputs = inputs.to(device)\n",
    "            encoded_labels = encoded_labels.to(device)\n",
    "            labels_len = labels_len.to(device)\n",
    "\n",
    "            outputs = model(inputs)  # (T, B, C)\n",
    "            logit_lens = torch.full(\n",
    "                size=(outputs.size(1),),\n",
    "                fill_value=outputs.size(0),\n",
    "                dtype=torch.long\n",
    "            ).to(device)\n",
    "\n",
    "            loss = criterion(outputs, encoded_labels, logit_lens, labels_len)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            pred_labels = torch.argmax(outputs, dim=2).permute(1, 0)  # (B, T)\n",
    "            pred_texts = vocab_util.decode(pred_labels)\n",
    "            target_texts = vocab_util.idx_2_labels(encoded_labels)\n",
    "\n",
    "            all_preds.extend(pred_texts)\n",
    "            all_targets.extend(target_texts)\n",
    "\n",
    "            for pred, target in zip(pred_texts, target_texts):\n",
    "                if pred == target:\n",
    "                    correct_count += 1\n",
    "\n",
    "            batch_cer = calculate_cer(pred_texts, target_texts)\n",
    "            pbar.set_postfix(loss=loss.item(), cer=batch_cer)\n",
    "\n",
    "    avg_loss = sum(losses) / len(losses)\n",
    "    final_cer = calculate_cer(all_preds, all_targets)\n",
    "    accuracy = correct_count / len(all_preds) if all_preds else 0.0\n",
    "\n",
    "    return avg_loss, final_cer, accuracy\n"
   ],
   "id": "d7912fee8ca4e72f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "evaluate_model(model, loader, device, ctc_loss, vocab)",
   "id": "95043a998026ef65"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Beam search",
   "id": "30daf43af0820036"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "def log_sum_exp(a, b):\n",
    "    if a == -float('inf'):\n",
    "        return b\n",
    "    if b == -float('inf'):\n",
    "        return a\n",
    "    return max(a, b) + math.log1p(math.exp(-abs(a - b)))\n",
    "\n",
    "def beam_search_decode(log_probs, beam_width=5, blank_index=1):\n",
    "    T, B, C = log_probs.size()\n",
    "    log_probs = log_probs.cpu()\n",
    "    final_results = []\n",
    "\n",
    "    for b in range(B):\n",
    "        beams = [([], 0.0)]\n",
    "        for t in range(T):\n",
    "            new_beams = {}\n",
    "            for prefix, score in beams:\n",
    "                for c in range(C):\n",
    "                    p = log_probs[t, b, c].item()\n",
    "                    new_prefix = prefix + [c]\n",
    "\n",
    "                    if len(prefix) > 0 and c == prefix[-1]:\n",
    "                        if c == blank_index:\n",
    "                            key = tuple(prefix)\n",
    "                            new_beams[key] = log_sum_exp(new_beams.get(key, -float('inf')), score + p)\n",
    "                        else:\n",
    "                            key = tuple(new_prefix)\n",
    "                            new_beams[key] = log_sum_exp(new_beams.get(key, -float('inf')), score + p)\n",
    "                    else:\n",
    "                        key = tuple(new_prefix)\n",
    "                        new_beams[key] = log_sum_exp(new_beams.get(key, -float('inf')), score + p)\n",
    "\n",
    "            beams = sorted(new_beams.items(), key=lambda x: x[1], reverse=True)[:beam_width]\n",
    "            beams = [(list(k), v) for k, v in beams]\n",
    "\n",
    "        best_seq = beams[0][0]\n",
    "        final_results.append(best_seq)\n",
    "\n",
    "    return final_results\n",
    "\n",
    "def calculate_cer(preds, targets):\n",
    "    import editdistance\n",
    "    total_dist = 0\n",
    "    total_len = 0\n",
    "    for p, t in zip(preds, targets):\n",
    "        total_dist += editdistance.eval(p, t)\n",
    "        total_len += len(t)\n",
    "    return total_dist / total_len if total_len > 0 else 0.0\n",
    "\n",
    "def calculate_acc(preds, targets):\n",
    "    correct = sum(p == t for p, t in zip(preds, targets))\n",
    "    return correct / len(targets) if targets else 0.0\n",
    "\n",
    "def evaluate_model(model, data_loader, device, criterion, vocab_util, epoch=None, beam_width=10):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    all_preds, all_targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(data_loader, desc=f\"Eval Epoch {epoch}\" if epoch else \"Evaluation\", leave=False)\n",
    "        for images, encoded_labels, labels_len in pbar:\n",
    "            images = images.to(device)\n",
    "            encoded_labels = encoded_labels.to(device)\n",
    "            labels_len = labels_len.to(device)\n",
    "\n",
    "            outputs = model(images)  # (T, B, C)\n",
    "            logit_lens = torch.full((outputs.size(1),), outputs.size(0), dtype=torch.long).to(device)\n",
    "\n",
    "            loss = criterion(outputs, encoded_labels, logit_lens, labels_len)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            log_probs = F.log_softmax(outputs, dim=2)\n",
    "            beam_decoded = beam_search_decode(log_probs, beam_width=beam_width, blank_index=1)\n",
    "\n",
    "            preds = vocab_util.decode(beam_decoded)\n",
    "            targets = vocab_util.idx_2_labels(encoded_labels)\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(targets)\n",
    "\n",
    "            cer = calculate_cer(preds, targets)\n",
    "            acc = calculate_acc(preds, targets)\n",
    "            pbar.set_postfix(loss=loss.item(), cer=cer, acc=acc)\n",
    "\n",
    "    return {\n",
    "        \"loss\": sum(losses) / len(losses),\n",
    "        \"cer\": calculate_cer(all_preds, all_targets),\n",
    "        \"acc\": calculate_acc(all_preds, all_targets),\n",
    "    }\n"
   ],
   "id": "5fdc5f27fc664bbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "evaluate_model(model, loader, device, ctc_loss, vocab)",
   "id": "cf7b24eb1be025f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#Upload",
   "id": "44d3d87295bb794d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load(os.path.join(save_path, \"best_model.pt\"), map_location=device)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "model.eval()\n",
    "\n",
    "# Upload widget\n",
    "uploader = widgets.FileUpload(accept='image/*', multiple=False)\n",
    "display(uploader)\n",
    "\n",
    "def predict_image(img_path):\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    image_tensor = test_transform(image).unsqueeze(0).to(device)  # (1, C, H, W)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor)  # (T, B, C)\n",
    "        pred_labels = torch.argmax(output, dim=2).permute(1, 0)  # (B=1, T)\n",
    "        print(pred_labels)\n",
    "        pred_texts = vocab.decode(pred_labels)\n",
    "\n",
    "    return pred_texts[0]\n",
    "\n",
    "def on_upload_change(change):\n",
    "    if uploader.value:\n",
    "        uploaded_file = next(iter(uploader.value.values()))\n",
    "        img_data = uploaded_file['content']\n",
    "        file_path = \"uploaded_image.jpg\"\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            f.write(img_data)\n",
    "\n",
    "        pred = predict_image(file_path)\n",
    "        img = Image.open(file_path)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Prediction: {pred}\")\n",
    "        plt.show()\n",
    "\n",
    "uploader.observe(on_upload_change, names='value')\n"
   ],
   "id": "9b7c94b16f7abbc7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
